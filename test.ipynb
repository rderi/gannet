{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Genome>:\n",
      "\tepochs = 93\n",
      "\tbatch_size = 79\n",
      "\thidden_layers = 97\n",
      "\tneuron_cnt = [52 52 52 52 52 52  7  7  7  7 22 22 22 22 22 83 83 83 83 83 83 83 83 83\n",
      " 83 83 83 83 83 83 83 83 83 83 83 83 83 83  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 14 14 14 14 99 99 99 99 99 99 99 99 99 99\n",
      " 99]\n",
      "\tactivation_functions = [<class 'torch.nn.modules.activation.SELU'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.Sigmoid'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Softmin'>, <class 'torch.nn.modules.activation.Sigmoid'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.GELU'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.SELU'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.Sigmoid'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Sigmoid'>, <class 'torch.nn.modules.activation.Sigmoid'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.SELU'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Sigmoid'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.Softmin'>, <class 'torch.nn.modules.activation.Softmin'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.SELU'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.activation.GELU'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.Sigmoid'>, <class 'torch.nn.modules.activation.SELU'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.activation.GELU'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.Sigmoid'>, <class 'torch.nn.modules.activation.Sigmoid'>, <class 'torch.nn.modules.activation.SELU'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Softmin'>, <class 'torch.nn.modules.activation.GELU'>, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.activation.SELU'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.ELU'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.PReLU'>]\n"
     ]
    }
   ],
   "source": [
    "import genomes\n",
    "import genome_parameters\n",
    "\n",
    "a = genomes.Genome(genome_parameters.GenomeParameters())\n",
    "print(str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': <genome_parameters.GenomeParameters at 0x1744a85f0>,\n",
       " 'epochs': 93,\n",
       " 'batch_size': 79,\n",
       " 'hidden_layers': 97,\n",
       " 'activation_fns': [torch.nn.modules.activation.SELU,\n",
       "  torch.nn.modules.activation.LeakyReLU,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.linear.Linear,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.ReLU,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.linear.Linear,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.linear.Linear,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.LeakyReLU,\n",
       "  torch.nn.modules.activation.Sigmoid,\n",
       "  torch.nn.modules.activation.Tanh,\n",
       "  torch.nn.modules.activation.Softmin,\n",
       "  torch.nn.modules.activation.Sigmoid,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.activation.GELU,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.activation.Tanh,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.Tanh,\n",
       "  torch.nn.modules.activation.SELU,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.Sigmoid,\n",
       "  torch.nn.modules.activation.Tanh,\n",
       "  torch.nn.modules.activation.Sigmoid,\n",
       "  torch.nn.modules.activation.Sigmoid,\n",
       "  torch.nn.modules.linear.Linear,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.LeakyReLU,\n",
       "  torch.nn.modules.activation.Tanh,\n",
       "  torch.nn.modules.activation.SELU,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.activation.LeakyReLU,\n",
       "  torch.nn.modules.activation.Tanh,\n",
       "  torch.nn.modules.activation.Sigmoid,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.Softmin,\n",
       "  torch.nn.modules.activation.Softmin,\n",
       "  torch.nn.modules.activation.LeakyReLU,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.SELU,\n",
       "  torch.nn.modules.linear.Linear,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.activation.ReLU,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.activation.ReLU,\n",
       "  torch.nn.modules.activation.GELU,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.LeakyReLU,\n",
       "  torch.nn.modules.activation.Sigmoid,\n",
       "  torch.nn.modules.activation.SELU,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.activation.GELU,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.activation.ReLU,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.activation.Sigmoid,\n",
       "  torch.nn.modules.activation.Sigmoid,\n",
       "  torch.nn.modules.activation.SELU,\n",
       "  torch.nn.modules.activation.Tanh,\n",
       "  torch.nn.modules.activation.Softmin,\n",
       "  torch.nn.modules.activation.GELU,\n",
       "  torch.nn.modules.activation.ReLU,\n",
       "  torch.nn.modules.activation.SELU,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.linear.Linear,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.Tanh,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.linear.Linear,\n",
       "  torch.nn.modules.activation.ReLU,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.Hardswish,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.ELU,\n",
       "  torch.nn.modules.activation.LeakyReLU,\n",
       "  torch.nn.modules.activation.PReLU,\n",
       "  torch.nn.modules.activation.Tanh,\n",
       "  torch.nn.modules.activation.Softmax,\n",
       "  torch.nn.modules.activation.PReLU],\n",
       " 'neuron_cnt': array([52, 52, 52, 52, 52, 52,  7,  7,  7,  7, 22, 22, 22, 22, 22, 83, 83,\n",
       "        83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83,\n",
       "        83, 83, 83, 83,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n",
       "        40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 14, 14, 14,\n",
       "        14, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 55, 72, 45, 25, 11, 40, 77, 94, 18, 53, 64, 88,  7, 73, 33, 66,\n",
       "       10, 73,  8, 63, 13, 69, 14, 78])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.neuron_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import genomes\n",
    "import neural_network\n",
    "\n",
    "a = genomes.Genome()\n",
    "nnet = neural_network.NeuralNetwork(a, input_layer=(nn.Linear, 28*28), output_layer=(nn.Linear, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[<Genome>:\\n\\tepochs = 3\\n\\tbatch_size = 20\\n\\thidden_layers = 21\\n\\tneuron_cnt = [71 71 71 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76]\\n\\tactivation_functions = [<class 'torch.nn.modules.activation.GELU'>, <class 'torch.nn.modules.activation.Hardswish'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.SELU'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.Softmin'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Softmin'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.activation.LeakyReLU'>, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.activation.Softmin'>, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.activation.SELU'>, <class 'torch.nn.modules.activation.Softmin'>, <class 'torch.nn.modules.activation.Softmin'>]\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (layers): Sequential(\n",
       "    (input): Linear(in_features=784, out_features=71, bias=True)\n",
       "    (GELU0): GELU(approximate='none')\n",
       "    (Hardswish0): Hardswish()\n",
       "    (Linear0): Linear(in_features=71, out_features=76, bias=True)\n",
       "    (LeakyReLU0): LeakyReLU(negative_slope=0.01)\n",
       "    (SELU0): SELU()\n",
       "    (Softmax0): Softmax(dim=None)\n",
       "    (Softmin0): Softmin(dim=None)\n",
       "    (Softmax1): Softmax(dim=None)\n",
       "    (Tanh0): Tanh()\n",
       "    (Softmin1): Softmin(dim=None)\n",
       "    (Tanh1): Tanh()\n",
       "    (Softmax2): Softmax(dim=None)\n",
       "    (LeakyReLU1): LeakyReLU(negative_slope=0.01)\n",
       "    (PReLU0): PReLU(num_parameters=1)\n",
       "    (LeakyReLU2): LeakyReLU(negative_slope=0.01)\n",
       "    (ReLU0): ReLU()\n",
       "    (Softmin2): Softmin(dim=None)\n",
       "    (ReLU1): ReLU()\n",
       "    (SELU1): SELU()\n",
       "    (Softmin3): Softmin(dim=None)\n",
       "    (output): Linear(in_features=76, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nn.Linear(2, 5)\n",
    "isinstance(c, nn.Linear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
